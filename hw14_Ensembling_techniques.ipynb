{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Advanced ensembling techniques\n","\n","Improve results on the dataset from previous homework with:\n","1. Simple ensembling approaches (mean, median, etc.)\n","2. Stacking"]},{"cell_type":"markdown","metadata":{"id":"WTUdl9BV9Op0"},"source":["Airbnb New User Bookings from https://www.kaggle.com/competitions/airbnb-recruiting-new-user-bookings. Our task is to predict which country a new user's first booking destination will be."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ymU1obrM9Opz"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import xgboost as xgb\n","import lightgbm as lgb\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","\n","from zipfile import ZipFile\n","\n","skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n","np.random.seed(24)\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["with ZipFile('./data/airbnb-recruiting/train_fe_session.zip') as zip_file:\n","    train = pd.read_csv(zip_file.extract('train_fe_session.csv'))\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_train_test(df):\n","    df = df.copy()\n","    X = df.drop(columns=['country_destination', 'id'])\n","    y = df['country_destination']\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=24)\n","    return X_train, X_test, y_train, y_test\n","\n","\n","def get_feat_types(df):\n","    numeric_features = df.select_dtypes('number').columns\n","    categorical_features = df.select_dtypes('object').columns\n","    return numeric_features, categorical_features\n","\n","\n","def get_train_test_transform(df, get_test=False):\n","    df = df.copy()\n","    X_train, X_test, y_train, y_test = get_train_test(df)\n","    numeric_features, categorical_features = get_feat_types(X_train)\n","\n","    numeric_transformer = Pipeline([\n","        ('imputer', SimpleImputer(strategy='median'))\n","        ])\n","\n","    categorical_transformer = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('enc', OrdinalEncoder(handle_unknown='use_encoded_value',\n","                               unknown_value=-1))])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numeric_transformer, numeric_features),\n","            ('cat', categorical_transformer, categorical_features)],\n","            verbose_feature_names_out=False)\n","\n","    preprocessor.set_output(transform='pandas')\n","\n","    X_train_transform = preprocessor.fit_transform(X_train)\n","\n","    y_transformer = LabelEncoder()\n","    y_train_enc = y_transformer.fit_transform(y_train)\n","\n","    if get_test == True:\n","        X_test_transform = preprocessor.transform(X_test)\n","        y_test_enc = y_transformer.transform(y_test)\n","        return X_train_transform, X_test_transform, y_train_enc, y_test_enc\n","\n","    return X_train_transform, y_train_enc\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["array(['NDF', 'US', 'other', 'FR', 'CA', 'GB', 'ES', 'IT', 'PT', 'NL',\n","       'DE', 'AU'], dtype=object)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["target_names = train['country_destination'].unique()\n","target_names\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["X_train_transform, y_train_enc = get_train_test_transform(train)\n"]},{"cell_type":"markdown","metadata":{"id":"5ZpKde5f9Op9"},"source":["### XGBoost model"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0]\ttrain-merror:0.39005+0.00005\ttest-merror:0.39002+0.00036\n","[50]\ttrain-merror:0.35189+0.00026\ttest-merror:0.35394+0.00097\n","[100]\ttrain-merror:0.34702+0.00067\ttest-merror:0.35191+0.00083\n","[133]\ttrain-merror:0.34484+0.00076\ttest-merror:0.35158+0.00106\n"]}],"source":["num_rounds = 1_000\n","\n","parameters_xgb = {\n","    \"objective\": \"multi:softprob\",\n","    \"num_class\": 12,\n","    \"eta\": 0.1,\n","    \"verbosity\": 1,\n","    \"seed\": 24,\n","    \"eval_metric\": \"merror\",\n","    \"tree_method\": \"hist\",\n","    \"grow_policy\": \"lossguide\",\n","\n","    # regularization parameters\n","    \"max_depth\": 10,\n","    \"max_leaves\": 15,\n","    \"subsample\": 0.7,\n","    \"colsample_bytree\": 0.6,\n","}\n","\n","xgb_train = xgb.DMatrix(X_train_transform, y_train_enc)\n","\n","results = xgb.cv(parameters_xgb, xgb_train, num_rounds,\n","                 folds=skf, early_stopping_rounds=10, verbose_eval=50)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["parameters_xgb = {\n","    \"n_estimators\":135,\n","    \"objective\": \"multi:softprob\",\n","    \"num_class\": 12,\n","    \"learning_rate\": 0.1,\n","    \"verbosity\": 1,\n","    \"random_state\": 24,\n","    \"eval_metric\": \"merror\",\n","    \"tree_method\": \"hist\",\n","    \"grow_policy\": \"lossguide\",\n","\n","    # regularization parameters\n","    \"max_depth\": 10,\n","    \"max_leaves\": 15,\n","    \"subsample\": 0.7,\n","    \"colsample_bytree\": 0.6,\n","}\n","\n","model_xgb = xgb.XGBClassifier(**parameters_xgb)\n"]},{"cell_type":"markdown","metadata":{},"source":["### LightGBM model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 10 rounds\n","[50]\tcv_agg's train multi_error: 0.346867 + 0.000438177\tcv_agg's valid multi_error: 0.353473 + 0.00097905\n","Early stopping, best iteration is:\n","[62]\tcv_agg's train multi_error: 0.344563 + 0.000628837\tcv_agg's valid multi_error: 0.353074 + 0.00114784\n"]}],"source":["num_rounds = 1_000\n","\n","parameters_lgb = {\n","    \"objective\": \"multiclass\",\n","    \"num_class\": 12,\n","    \"learning_rate\": 0.1,\n","    \"num_leaves\": 15,\n","    \"num_threads\": 4,\n","    \"seed\": 24,\n","    \"metric\": \"multi_error\",\n","    \"verbose\": 0,\n","    \"force_row_wise\": True,\n","    \"is_unbalance\": True,\n","\n","    #regularization\n","    \"colsample_bytree\": 0.6,\n","    \"subsample\": 0.7,\n","    \"subsample_freq\": 1,\n","    \"min_data_in_leaf\": 25\n","}\n","\n","\n","X_train_transform, y_train_enc = get_train_test_transform(train)\n","\n","lgb_train = lgb.Dataset(X_train_transform, y_train_enc)\n","\n","result = lgb.cv(\n","    parameters_lgb, lgb_train, num_rounds, folds=skf,\n","    eval_train_metric=True,\n","    callbacks=[lgb.early_stopping(10), lgb.log_evaluation(50)]\n","    )\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["parameters_lgb = {\n","    \"n_estimators\": 60,\n","    \"objective\": \"multiclass\",\n","    \"num_class\": 12,\n","    \"learning_rate\": 0.1,\n","    \"num_leaves\": 15,\n","    \"n_jobs\": 4,\n","    \"random_state\": 24,\n","    \"metric\": \"multi_error\",\n","    \"verbose\": 0,\n","    \"force_row_wise\": True,\n","    \"is_unbalance\": True,\n","\n","    #regularization\n","    \"colsample_bytree\": 0.6,\n","    \"subsample\": 0.7,\n","    \"subsample_freq\": 1,\n","    \"min_child_samples\": 25\n","}\n","\n","model_lgb = lgb.LGBMClassifier(**parameters_lgb)\n"]},{"cell_type":"markdown","metadata":{},"source":["### ExtraTreesClassifier model"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def cv_results(model, X, y):\n","    results = cross_validate(model, X, y, scoring='accuracy',\n","                            cv=skf, return_train_score=True)\n","    df_results = pd.DataFrame(results)\n","    df_results = df_results[['test_score', 'train_score']].apply(lambda x: 1-x)\n","    return df_results\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["X_train_transform, X_test_transform, y_train_enc, y_test_enc = get_train_test_transform(train, get_test=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.415900</td>\n","      <td>0.415416</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.415039</td>\n","      <td>0.415188</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.415531</td>\n","      <td>0.415452</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   test_score  train_score\n","0    0.415900     0.415416\n","1    0.415039     0.415188\n","2    0.415531     0.415452"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["model_etc = ExtraTreesClassifier(max_depth=5, max_features=10,\n","                                 min_samples_leaf=3, random_state=24)\n","\n","cv_results(model_etc, X_train_transform, y_train_enc)\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.364248</td>\n","      <td>0.348674</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.364652</td>\n","      <td>0.350395</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.363176</td>\n","      <td>0.350940</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   test_score  train_score\n","0    0.364248     0.348674\n","1    0.364652     0.350395\n","2    0.363176     0.350940"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["model_etc = ExtraTreesClassifier(max_depth=10, max_features=100,\n","                                 min_samples_leaf=3, random_state=24)\n","\n","cv_results(model_etc, X_train_transform, y_train_enc)\n"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.362614</td>\n","      <td>0.347874</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.362193</td>\n","      <td>0.348296</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.361314</td>\n","      <td>0.348955</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   test_score  train_score\n","0    0.362614     0.347874\n","1    0.362193     0.348296\n","2    0.361314     0.348955"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["model_etc = ExtraTreesClassifier(max_depth=10, max_features=150,\n","                                 min_samples_leaf=5, random_state=24)\n","\n","cv_results(model_etc, X_train_transform, y_train_enc)\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_score</th>\n","      <th>train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.358117</td>\n","      <td>0.265021</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.357203</td>\n","      <td>0.263879</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.355692</td>\n","      <td>0.267103</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   test_score  train_score\n","0    0.358117     0.265021\n","1    0.357203     0.263879\n","2    0.355692     0.267103"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["model_etc = ExtraTreesClassifier(max_depth=25, max_features=150,\n","                                 min_samples_leaf=10, random_state=24)\n","\n","cv_results(model_etc, X_train_transform, y_train_enc)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model_etc = ExtraTreesClassifier(max_depth=10, max_features=100,\n","                                 min_samples_leaf=3, random_state=24)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Ensembling"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["xgb_prediction = []\n","lgb_prediction = []\n","etc_prediction = []\n","\n","for train, val in skf.split(X_train_transform, y_train_enc):\n","    model_xgb.fit(X_train_transform.iloc[train], y_train_enc[train])\n","    model_lgb.fit(X_train_transform.iloc[train], y_train_enc[train])\n","    model_etc.fit(X_train_transform.iloc[train], y_train_enc[train])\n","\n","    xgb_prediction.append([y_train_enc[val], model_xgb.predict_proba(X_train_transform.iloc[val])])\n","    lgb_prediction.append([y_train_enc[val], model_lgb.predict_proba(X_train_transform.iloc[val])])\n","    etc_prediction.append([y_train_enc[val], model_etc.predict_proba(X_train_transform.iloc[val])])\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["xgb model, accuracy:  0.6477746544858279\n","lgb model, accuracy:  0.6462871866947763\n","etc model, accuracy:  0.6359744670883111\n"]}],"source":["print('xgb model, accuracy: ', np.mean([accuracy_score(i[0], np.argmax(i[1], axis=1)) for i in xgb_prediction]))\n","print('lgb model, accuracy: ', np.mean([accuracy_score(i[0], np.argmax(i[1], axis=1)) for i in lgb_prediction]))\n","print('etc model, accuracy: ', np.mean([accuracy_score(i[0], np.argmax(i[1], axis=1)) for i in etc_prediction]))\n"]},{"cell_type":"markdown","metadata":{},"source":["### Averaging"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/plain":["0.6480030452096509"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]/3 + lgb_p[1]/3 + etc_p[1]/3), axis=1))\n","     for xgb_p, lgb_p, etc_p in zip(xgb_prediction, lgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["0.6484012649332396"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.5 + lgb_p[1]*0.5), axis=1))\n","     for xgb_p, lgb_p in zip(xgb_prediction, lgb_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["0.6466151323494964"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.5 + etc_p[1]*0.5), axis=1))\n","     for xgb_p, etc_p in zip(xgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["0.6437456078706957"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(lgb_p[0], np.argmax((lgb_p[1]*0.5 + etc_p[1]*0.5), axis=1))\n","     for lgb_p, etc_p in zip(lgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Geometric mean"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["0.6478332162098853"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1] * lgb_p[1] * etc_p[1])**(1/3), axis=1))\n","     for xgb_p, lgb_p, etc_p in zip(xgb_prediction, lgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"text/plain":["0.6486765050363082"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1] * lgb_p[1])**(1/2), axis=1))\n","     for xgb_p, lgb_p in zip(xgb_prediction, lgb_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["0.6468435230733193"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1] * etc_p[1])**(1/2), axis=1))\n","     for xgb_p, etc_p in zip(xgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["0.6442433825251815"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(lgb_p[0], np.argmax((lgb_p[1] * etc_p[1])**(1/2), axis=1))\n","     for lgb_p, etc_p in zip(lgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Weighted average"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"data":{"text/plain":["0.6479444834855937"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.35 + lgb_p[1]*0.35 + etc_p[1]*0.3), axis=1))\n","     for xgb_p, lgb_p, etc_p in zip(xgb_prediction, lgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"data":{"text/plain":["0.648190442726634"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.4 + lgb_p[1]*0.4 + etc_p[1]*0.2), axis=1))\n","     for xgb_p, lgb_p, etc_p in zip(xgb_prediction, lgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"data":{"text/plain":["0.6483661278988053"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.45 + lgb_p[1]*0.45 + etc_p[1]*0.1), axis=1))\n","     for xgb_p, lgb_p, etc_p in zip(xgb_prediction, lgb_prediction, etc_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"data":{"text/plain":["0.6486179433122512"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.6 + lgb_p[1]*0.4), axis=1))\n","     for xgb_p, lgb_p in zip(xgb_prediction, lgb_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"data":{"text/plain":["0.6485769501054112"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.7 + lgb_p[1]*0.3), axis=1))\n","     for xgb_p, lgb_p in zip(xgb_prediction, lgb_prediction)]\n",")\n"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"data":{"text/plain":["0.6487526352775825"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(\n","    [accuracy_score(xgb_p[0], np.argmax((xgb_p[1]*0.57 + lgb_p[1]*0.43), axis=1))\n","     for xgb_p, lgb_p in zip(xgb_prediction, lgb_prediction)]\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["The best result 0.64875 for weighted average of two model: XGBoost and LightGBM. "]},{"cell_type":"markdown","metadata":{},"source":["## Blending\n","\n","1. Divide a training dataset on a new train and hold out.\n","2. Train n-models on the new training dataset and make predictions on hold-out.\n","3. Concatenate all hold-out predictions and train a new meta-model on this data.\n","4. Make predictions on test data with models trained on the \"new train\" dataset.\n","5. Concatenate these predictions and use a meta-model to get final predictions.\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Divide dataset on train and test\n","X_train_val, X_test, y_train_val, y_test = get_train_test(train)\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# 1. Divide a training dataset on a new train and hold out\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,\n","                                                  stratify=y_train_val,\n","                                                  test_size=0.2, random_state=24)\n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["def pipeline(X_train, y_train, model):\n","    numeric_features, categorical_features = get_feat_types(X_train)\n","\n","    numeric_transformer = Pipeline([\n","        ('imputer', SimpleImputer(strategy='median'))\n","        ])\n","\n","    categorical_transformer = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('enc', OrdinalEncoder(handle_unknown='use_encoded_value',\n","                               unknown_value=-1))])\n","\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('num', numeric_transformer, numeric_features),\n","            ('cat', categorical_transformer, categorical_features)],\n","            verbose_feature_names_out=False)\n","\n","    preprocessor.set_output(transform='pandas')\n","\n","    pipeline = Pipeline([\n","        ('prepros', preprocessor),\n","        ('classifier', model)\n","        ])\n","\n","    pipeline.fit(X_train, y_train)\n","\n","    return pipeline\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["y_transformer = LabelEncoder()\n","y_transformer.fit(y_train)\n","y_train_enc = y_transformer.transform(y_train)\n","y_val_enc = y_transformer.transform(y_val)\n","y_test_enc = y_transformer.transform(y_test)\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# 2. Train all models on the new training dataset and make predictions on hold-out\n","model_xgb_fit = pipeline(X_train, y_train_enc, model_xgb)\n","pred_xgb = model_xgb_fit.predict_proba(X_val)\n","\n","model_lgb_fit = pipeline(X_train, y_train_enc, model_lgb)\n","pred_lgb = model_lgb_fit.predict_proba(X_val)\n","\n","model_etc_fit = pipeline(X_train, y_train_enc, model_etc)\n","pred_etc = model_etc_fit.predict_proba(X_val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 3.1 Concatenate all hold-out predictions\n","stack_val_preds = np.column_stack((pred_xgb, pred_lgb, pred_etc))\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# specify meta model\n","meta_model = LogisticRegression(max_iter=1000, multi_class='multinomial', random_state=24, C=0.3)\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["# 3.2 Train a new meta-model on concatenated predictions\n","meta_model.fit(stack_val_preds, y_val_enc)\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# 4. Make predictions on test data with models trained on the \"new train\" dataset\n","test_pred_xgb = model_xgb_fit.predict_proba(X_test)\n","\n","test_pred_lgb = model_lgb_fit.predict_proba(X_test)\n","\n","test_pred_etc = model_etc_fit.predict_proba(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 5. Concatenate these predictions and use a meta-model to get final predictions\n","stack_test_preds = np.column_stack((test_pred_xgb, test_pred_lgb, test_pred_etc))\n","\n","predictions = meta_model.predict_proba(stack_test_preds)\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy on cross validation:  0.650474349964863\n","accuracy on validation:  0.6505914734129773\n","accuracy on test:  0.6496685484059872\n"]}],"source":["print('accuracy on cross validation: ', np.mean(cross_val_score(meta_model, stack_val_preds,\n","                                                         y_val_enc, cv=skf, scoring='accuracy')))\n","print('accuracy on validation: ', meta_model.score(stack_val_preds, y_val_enc))\n","print('accuracy on test: ', meta_model.score(stack_test_preds, y_test_enc))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Stacking\n","\n","1. Split train data on n-folds.\n","2. Train on each fold a model and make predictions for a hold-out fold.\n","3. Concatenate hold-out folds predictions and add them as a new meta-feature. Train a final meta-model.\n","4. Train a first-level model on the whole train data set.\n","5. Make predictions on a test set with the first-level model. Get your meta-feature for the test set.\n","6. Concatenate the meta-feature to the test set. Use the meta-model from step 3 to make final predictions"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Divide dataset on train and test\n","X_train, X_test, y_train, y_test = get_train_test(train)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["y_transformer = LabelEncoder()\n","y_transformer.fit(y_train)\n","y_train_enc = y_transformer.transform(y_train)\n","y_test_enc = y_transformer.transform(y_test)\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["xgb_prediction = []\n","lgb_prediction = []\n","etc_prediction = []\n","\n","# 1. Split train data on n-folds\n","for i, (train, val) in enumerate(skf.split(X_train, y_train_enc)):\n","    # 2. Train on each fold a model and make predictions for a hold-out fold\n","    model_xgb_fit = pipeline(X_train.iloc[train], y_train_enc[train], model_xgb)\n","    model_lgb_fit = pipeline(X_train.iloc[train], y_train_enc[train], model_lgb)\n","    model_etc_fit = pipeline(X_train.iloc[train], y_train_enc[train], model_etc)\n","\n","    xgb_prediction.append([y_train_enc[val], model_xgb_fit.predict_proba(X_train.iloc[val])])\n","    lgb_prediction.append([y_train_enc[val], model_lgb_fit.predict_proba(X_train.iloc[val])])\n","    etc_prediction.append([y_train_enc[val], model_etc_fit.predict_proba(X_train.iloc[val])])\n","\n","    # 3.1 Concatenate hold-out folds predictions and add them as a new meta-feature\n","    X_train_new = X_train.copy()\n","    for j in range(12):\n","        X_train_new.loc[X_train.index[val], f'xgb_{j}'] = xgb_prediction[i][1][:, j]\n","        X_train_new.loc[X_train.index[val], f'lgb_{j}'] = lgb_prediction[i][1][:, j]\n","        X_train_new.loc[X_train.index[val], f'etc_{j}'] = etc_prediction[i][1][:, j]\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# specify meta model\n","parameters_xgb = {\n","    \"n_estimators\":135,\n","    \"objective\": \"multi:softprob\",\n","    \"num_class\": 12,\n","    \"learning_rate\": 0.1,\n","    \"verbosity\": 1,\n","    \"random_state\": 24,\n","    \"eval_metric\": \"merror\",\n","    \"tree_method\": \"hist\",\n","    \"grow_policy\": \"lossguide\",\n","\n","    # regularization parameters\n","    \"max_depth\": 10,\n","    \"max_leaves\": 15,\n","    \"subsample\": 0.7,\n","    \"colsample_bytree\": 0.6,\n","}\n","\n","meta_model = xgb.XGBClassifier(**parameters_xgb)\n","\n","# 3.2 Train a final meta-model\n","meta_model_fit = pipeline(X_train_new, y_train_enc, meta_model)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# 4. Train a first-level model on the whole train data set\n","model_xgb_fit_new = pipeline(X_train, y_train_enc, model_xgb)\n","model_lgb_fit_new = pipeline(X_train, y_train_enc, model_lgb)\n","model_etc_fit_new = pipeline(X_train, y_train_enc, model_etc)\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# 5. Make predictions on a test set with the first-level model.\n","# Get your meta-feature for the test set\n","test_pred_xgb = model_xgb_fit_new.predict_proba(X_test)\n","test_pred_lgb = model_lgb_fit_new.predict_proba(X_test)\n","test_pred_etc = model_etc_fit_new.predict_proba(X_test)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# 6.1 Concatenate the meta-feature to the test set.\n","X_test_new = X_test.copy()\n","for j in range(12):\n","    X_test_new[f'xgb_{j}'] = test_pred_xgb[:, j]\n","    X_test_new[f'lgb_{j}'] = test_pred_lgb[:, j]\n","    X_test_new[f'etc_{j}'] = test_pred_etc[:, j]\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# 6.2 Use the meta-model from step 3 to make final predictions\n","predictions = meta_model_fit.predict_proba(X_test_new)\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["accuracy on cross validation:  0.6454673225579761\n","accuracy on train:  0.6519793862731319\n","accuracy on test:  0.6472558618912652\n"]}],"source":["print('accuracy on cross validation: ', np.mean(cross_val_score(\n","    meta_model_fit, X_train_new, y_train_enc, cv=skf, scoring='accuracy')))\n","print('accuracy on train: ', meta_model_fit.score(X_train_new, y_train_enc))\n","print('accuracy on test: ', meta_model_fit.score(X_test_new, y_test_enc))\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"11yEOBs6SO9VzerMLwo9nT5xWiiKhQj1x","timestamp":1684241080629}]},"kernelspec":{"display_name":"projector","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
